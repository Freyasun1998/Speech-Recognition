# Lab-11: Speech Recognition

This Lab provides a brief introduction to open source speech recognition tools from the [Huggingface library](https://huggingface.co/transformers/) and from [NVIDIA's NeMO toolkit](https://github.com/NVIDIA/NeMo). The graded portion of this lab can be found below. In it we will transcribe and audio recording of our voice using the [Wave2Vec2.0 model](https://huggingface.co/transformers/model_doc/wav2vec2.html), and then evaluate the quality of the transcription using [word](https://huggingface.co/metrics/wer) and [character](https://huggingface.co/metrics/cer) error rates. If you'd like to dig deeper, there is a tutorial (`lab-11.ipynb`) from NVIDIA NeMo that will give you an idea of what goes into online voice recognition, which requires us to stream an audio signal directly from a microphone into the model running in your python process. This Jupyter notebook will not be graded.

## Speech recognition with Wave2Vec2.0


### Task I (20 pts)

Take a look at `asr.py`. In it you'll find four different modes: record, play, transcribe, evaluate. To get started, issue the following command:

    $ python asr.py record myvoice.wav 30

# ans: see myvoice.wav
    
Following the prompt from the terminal, read aloud the text in `ground-truth.txt`. To make sure your voice was recorded properly, run the script in *play* mode:

    $ python asr.py play myvoice.wav 30
    
Next, we're going to transcribe this audio file using the Wave2Vec2.0 base model from Huggingface. Issue the following command:

    $ python asr.py transcribe myvoice.wav

# ans: see myvoice_transcription.txt
    
Finally, we need to evaluate the quality of our model's transcription by measuring the word error rate (WER) and character error rate (CER). Issue the following command:

    $ python asr.py evaluate myvoice_transcription.txt ground-truth.txt

# ans:
#    "word_error_rate": 56.1404,
#   "character_error_rate": 23.3244

This command will produce a `results.json` file containing the WER and CER metrics metrics. Submit the resultant audio file, transcription, and results files to your github repo.


### Task II (10 pts)

1. How would you grade the models transcriptions of your voice (without considering WER)?

# ans: 
Not as good as I thought. The model did well in recognizing some keywords, like the "rules about how paymen amounts correspond to follower conts". However, it depends too much on the pronunciation with considering some complex language structure. For example, "instegram's tales", which should be "Instagram tells".

2. Why is the character level error rate lower than at the word level?

# ans
Technically, both CER and WER are calculated by "(S + D + I) / N = (S + D + I) / (S + D + C)". It is harder to predict the right word than the right character as there are only 26 characters and only a few pairs with similar pronunciations, like "t" and "d" in a word. However, there may be many words with similar pronunciations, like "tale" and "tell".

Then, it is easier to truncate the length of a character than the length of the word. Most monosyllabic characters (only one syllable) can be easily recognized within a certain length, while the length of a word needs to be determined in various ways. This increases the probability of recognizing wrong words.

Finally, when converting the sound waves to the numerical matrix data, there may be quality lost due to the accent for different people.


3. Are the models errors primarily on the word level, or is it missing phonetic sounds all together? Given this assessment, if you had to improve this model, would you focus primarily on the acoustic model or the language model?

# ans:
The above measurements (CER and WER) are on the word level. For example, CER = (S + D + I) / N = (S + D + I) / (S + D + C) where S is the number of substitutions, D is the number of deletions, I is the number of insertions, C is the number of correct characters, N is the number of characters in the reference (N=S+D+C). The formula is primarily on the percentage of characters that were incorrectly predicted.

However, when measuring the model, we should depend on an integrated way on both word-level and phonetic sounds level together.

To improve this model, I would focus primarily on the acoustic model. Compared with the language model, it is more difficult to train an acoustic model because of various reasons, like the expensive qualified voice data. Currently, the accuracy of the language model can achieve high performance while the accuracy of the acoustic model is relatively low. Once we can convert the sound wave into the text well, it will enhance the overall accuracy of the model a lot.


